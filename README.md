# README — Проект по предсказанию ddG на основе базы SKEMPI_v2

## Цель проекта
Создать модель машинного обучения, способную предсказывать знак изменения свободной энергии связывания (ddG) при мутации в белках, на основе экспериментальных данных из базы **SKEMPI_v2.0.csv**.

## Что реализовано
- Загрузка и очистка данных (приведение типов, фильтрация).
- Вычисление ddG на основе уравнения Гиббса: `ΔG = ΔH - T·ΔS` ().
- Вывод целевой переменной `ddG_sign`.
- Извлечение признаков, включая логарифмы аффинитетов и дельты энтальпии/энтропии.
- Фильтрация выбросов (IQR + Z-score).
- Построение sklearn-пайплайна с LightGBM, StandardScaler, SimpleImputer и OneHotEncoder.
- Поддержка GridSearchCV.
- Оценка модели (accuracy, ROC AUC, кросс-валидация, важность признаков).
- Все визуализации собраны в одно окно.

## Возникшие сомнения и размышления

### 1. **Какие значения считать ddG?**
Сначала было неясно, что именно использовать: аффинитеты, ΔH или ΔG? В исходной базе `Affinity_*` интерпретировались как ΔH, но на практике это скорее свободная энергия связывания. Использована формула:

```ddG = (Affinity_mut - Affinity_wt) - T * (dS_mut - dS_wt)```

Это приближённая форма ΔΔG = ΔG_mut - ΔG_wt. Я решил, что она адекватна в контексте задачи.

### 2. **Почему остаётся мало строк после очистки?**
При первой реализации после всех фильтров (dropna + выбросы) оставалось <200 строк из 7085. Потом выяснилось:
- многие строки имеют пропуски в кинетических константах;
- логарифм от 0 → `NaN`.

Добавлен `SimpleImputer`, чтобы не терять информацию без необходимости.

### 3. **Что считать признаком, а что — целевой переменной?**
Были мысли использовать абсолютное значение ddG, но финально оставил **знак ddG** как классификационную метку, потому что:
- он устойчивее к шуму,
- биологически важно, улучшает ли мутация связывание или нет.

### 4. **Какие признаки важны?**
На раннем этапе добавлял `kon`, `koff`, но их очень много пропущено. В итоговой версии используются только стабильные признаки: лог аффинитета, энтальпия, энтропия.

### 5. **Почему LightGBM?**
Я перепробовал `RandomForest`, `LogisticRegression`, но `LightGBM` дал лучшее сочетание качества и интерпретируемости + быстрый GridSearch.

## Запуск

**python ddg_pipeline.py --data skempi_v2.csv**

Можно отключить поиск по сетке:
**python ddg_pipeline.py --data skempi_v2.csv --no-grid**


## Результат
- Accuracy > 98%
- ROC AUC близко к 1.0
- Топ-признаки визуализируются
- Поддерживается сохранение модели

## Вывод
Проект дал возможность на практике реализовать полный ML-пайплайн: от сырых биофизических данных до визуализации и сохранения модели. На каждом этапе возникали вопросы, требовавшие осознанных решений — от формулы Гиббса до фильтрации данных. Наиболее трудными были моменты выбора модели и фильтрации данных (выбросы, пропущенные строки) в связи с малым опытом. Это был не просто технический процесс, а настоящий исследовательский путь. #RandomizedSearchCV (гиперпораметры).
